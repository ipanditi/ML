{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMeRfWyldpfw7yYKJnOPmM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipanditi/ML/blob/main/spotify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NT0kXXFJr_md"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "\n",
        "dataset_path = \"genres\"\n",
        "json_path = \"data_json\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first define the number of segments and sample rate of each segment. The sample rate is required in order to know the playback speed of the song."
      ],
      "metadata": {
        "id": "_C4ZX-u9twzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_rate = 22050\n",
        "samples_per_track = sample_rate*30\n",
        "#Because 30 seconds is the length of each track"
      ],
      "metadata": {
        "id": "hFOd1n1Dt_dJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then create a loop in which we open up every song file from each genre folder and split it into 10 segments."
      ],
      "metadata": {
        "id": "O-_MGo5euh8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Link to learn about MFCC](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)"
      ],
      "metadata": {
        "id": "iBltGQzcvP7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segment=5):\n",
        "  #We then extract the MFCC features for each of that segment and append it to the dictionary under the genre name.\n",
        "  data = {\n",
        "      \"mapping\": [],\n",
        "      \"labels\": [],\n",
        "      \"mfcc\": []\n",
        "  }\n",
        "\n",
        "  #samples_per_segment = ((sample_rate*30)/number of segents)\n",
        "  samples_per_segment = int(samples_per_track / num_segment) \n",
        "  \n",
        "  #number of vectors using ceiling or greatest integer function\n",
        "  num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
        "\n",
        "  #Looping\n",
        "  for i,(dirpath,dirnames,filenames) in enumerate(os.walk(dataset_path)):\n",
        "\n",
        "    #check if the path is genres\n",
        "    if dirpath != dataset_path:\n",
        "\n",
        "      #add all the labels\n",
        "      label = str(dirpath).split('\\\\')[-1]\n",
        "      data[\"mapping\"].append(label)\n",
        "      print(\"\\nInside\", label)\n",
        "\n",
        "      #Going through each track within a created label\n",
        "      for f in filenames:\n",
        "        \n",
        "        #Address compatible with Windows OS\n",
        "        file_path = dataset_path+\"/\"+str(label)+\"/\"+ str(f)\n",
        "\n",
        "        #\n",
        "        y, sr = librosa.load(file_path, sr = sample_rate) \n",
        "\n",
        "        #Cutting each song into 5 segments(num_segments)\n",
        "        for n in range(num_segment):\n",
        "\n",
        "          #for 1 segment multiply with 1; similarly go on till 5\n",
        "          start = samples_per_segment*n \n",
        "\n",
        "          #finish = (samples_per_segment)(i+1)\n",
        "          finish  = start + samples_per_segment\n",
        "\n",
        "          #define mfcc; print(start, finish)\n",
        "          mfcc = librosa.feature.mfcc(y[start:finish], sample_rate, n_mfcc=num_mfcc, n_fft = n_fft, hop_length = hop_length)\n",
        "          mfcc = mfcc.T #259*13\n",
        "\n",
        "          #If length of mfcc is equal to total number of mfcc vectors we defined earlier\n",
        "          if len(mfcc) == num_mfcc_vectors_per_segment:\n",
        "\n",
        "            #append the following in the dictionary\n",
        "            data[\"mfcc\"].append(mfcc.tolist())\n",
        "            data[\"labels\"].append(i-1)\n",
        "            \n",
        "            #Print out the track name\n",
        "            print(\"Track Name :\", file_path, n+1)\n",
        "        \n",
        "  #Dump all the processed data into the json file using Write function(w)\n",
        "  with open(json_path, \"w\") as fp:\n",
        "    json.dump(data, fp, indent = 4)\n",
        "\n",
        "#The driving/ main function\n",
        "if __name__ == \"__main__\":\n",
        "  preprocess(dataset_path, json_path, num_segment=10)"
      ],
      "metadata": {
        "id": "OLZ7VEZcuj1J"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above script will create segments and extract features and dump the features into data_json.json file."
      ],
      "metadata": {
        "id": "gu85Fxty2oDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the model**\n",
        "We use the LSTM Recurrent neural networks [Link to Learn about LSTM model](https://courses.cognitiveclass.ai/courses/course-v1:BigDataUniversity+ML0120EN+v2/courseware/bd64ccdf56ad4ea1afe870e26d583038/8f960392dacc48bebb8230b9efad3f8b/?activate_block_id=block-v1%3ABigDataUniversity%2BML0120EN%2Bv2%2Btype%40sequential%2Bblock%408f960392dacc48bebb8230b9efad3f8b).  But before we build the model, we have to load the model into our program and split it into training and testing."
      ],
      "metadata": {
        "id": "DDGmTW1S9j7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import required modules\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "data_path = \"data_json\"\n",
        "\n",
        "#Load the data\n",
        "def load_data(data_path):\n",
        "  print(\"Data Loading\\n \")\n",
        "  with open(data_path, \"r\") as fp:\n",
        "    data = json.load(fp)\n",
        "\n",
        "  #define the dependent and the independent variables\n",
        "  #mfcc being the independent variable:\n",
        "  x = np.array(data[\"mfcc\"])\n",
        "\n",
        "  #labels being the dependent variable:\n",
        "  y = np.array(data[\"labels\"])\n",
        "\n",
        "  #This code snippet tells us that we are classifying labels/genres using their mfcc values\n",
        "  print(\"Loaded data:\")\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "CLFQvABNvvBm"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have loaded the data and labeled it accordingly, we now go onto split it into train and test datasets"
      ],
      "metadata": {
        "id": "7LqoTgmEAkMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_datasets(test_size, val_size):\n",
        "\n",
        "  #load\n",
        "  x, y = load_data(data_path)\n",
        "  #split\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= test_size, random_state=42)\n",
        "\n",
        "  #splitting the training set further:\n",
        "  x_train, x_val, x_train, y_val = train_test_split(x_train, y_train, test_size = val_size, random_state = 42)\n",
        "\n",
        "  #return the splitted dataset\n",
        "  return x_train, y_train, x_test, y_test , x_val, y_val\n"
      ],
      "metadata": {
        "id": "4fsN9IexAggL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we get into the Model building part.  the LSTM network is created using tensorflow. Here, we have created an LSTM network of 4 layers, including two hidden layers. The following code snippet shows the network creation."
      ],
      "metadata": {
        "id": "eoZEDK1OBx28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape):\n",
        "  # We use a sequential model since the data i.e, Sound is sequential in nature.\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  #Building the layers of the neural network:\n",
        "  model.add(tf.keras.layers.LSTM(64, input_shape = input_shape, return_sequences=True))\n",
        "  model.add(tf.keras.layers.LSTM(64))\n",
        "\n",
        "  #Define the density and activation function for the meural network:\n",
        "  model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
        "\n",
        "  #add another activation function to the end of the NN, which gives out the probability.\n",
        "  model.add(tf.keras.layers.Dense(10, activation=\"Softmax\"))\n",
        "  return model"
      ],
      "metadata": {
        "id": "XVpXpcvMBtgx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now write the driving function or the main function"
      ],
      "metadata": {
        "id": "TUVXlEkrEP2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  #call the prepare_dataset function with test_size = 0.25 and val_size = 0.2:\n",
        "  x_train, x_test, x_val, y_train, y_test, y_val = prepare_datasets(0.25, 0.2)\n",
        "\n",
        "  print(x_train.shape[0])\n",
        "  input_shape = (x_train.shape[1], x_train.shape[2])\n",
        "  model = build_model(input_shape)\n",
        "\n",
        "  #compile the model with teh learning rate as 0.001\n",
        "  optimiser = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "  #use cross_entropy as the loss function and accuracy as the testing metrics\n",
        "  model.compile(optimizer = optimiser, loss = 'sparse_categorical_cross_entropy', metrics = ['accuracy'])\n",
        "   \n",
        "  #summarize\n",
        "  model.summary()\n",
        "\n",
        "  #curve fitting\n",
        "  #model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=32, epochs=50)\n",
        "  #model.save(\"model_RNN_LSTM.h5\")\n",
        "  #print(\"Saved model to disk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "HaznoLmbEMiZ",
        "outputId": "c6e9d3b8-1de2-4ffc-e673-48ae3d707d69"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Loading\n",
            " \n",
            "Loaded data:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-60a0f8350058>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m#call the prepare_dataset function with test_size = 0.25 and val_size = 0.2:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-30283030446c>\u001b[0m in \u001b[0;36mprepare_datasets\u001b[0;34m(test_size, val_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m#splitting the training set further:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2419\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m     n_train, n_test = _validate_shuffle_split(\n\u001b[0;32m-> 2421\u001b[0;31m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2422\u001b[0m     )\n\u001b[1;32m   2423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2099\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m             \u001b[0;34m\"aforementioned parameters.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2102\u001b[0m         )\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "F0tVhw_aGZt5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}